{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Introduction to Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Resources for this notebook include:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapters 3 and 4.  \n",
    "- [Gordon Richard's notebooks](https://github.com/gtrichards/PHYS_T480)\n",
    "- random contributions from a large number of colleagues (e.g. Jake VanderPlas, Andy Connolly)\n",
    "\n",
    "##### Suggested supplemental background reading:\n",
    "\n",
    "[David Hogg: \"Data analysis recipes: Probability calculus for inference\"](https://arxiv.org/abs/1205.4446)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goals of Science\n",
    "\n",
    "What are our goals, as scientists?\n",
    "\n",
    "1. Describe the world around us.\n",
    "1. Predict outcomes.\n",
    "1. Explain the observations and predictions.\n",
    "\n",
    "Our goal here is to introduce you to techniques allowing you to quantitatively, reproducibly, and objectivelly reach those goals -- *build knowledge* -- from collected observations and conducted experiments (data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Science and probability\n",
    "\n",
    "* Most science is fundamentally *inductive*: based on a _limited_ set of specific observations, we attempt to generalize and infer the underlying natural laws (rules of cause and effect) that govern the observed phenomena.\n",
    "\n",
    "* This generalization naturally leads to uncertainty: e.g., as we haven't observed or tested all possible outcomes, it's always possible our model (theory) may fail under some circumstances.\n",
    "\n",
    "* That is fine. Science advances by rejecting (_falsifying_) theories when data is found that does not fit them. How do we know if the discrepancy is significant enough for a theory to be rejected?\n",
    "\n",
    "* To be able to objectively and reproducibly reason about these questions, we need to quantify uncertainty. It's matematical expression is that of *probability*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Mathematics of Probability\n",
    "\n",
    "We distinguish between ***events*** (e.g. \"we rolled an even number\") and a set of ***outcomes*** favorable to those events (e.g., \"2, 4, 6\").\n",
    "\n",
    "We'll adopt a working definition of probability $P(A)$ as **the ratio of the number of outcomes favorable to event A over the number of total possible outcomes**. If the outcomes are continuous, $p(x) dx$ will denote the probability of events in an infinitesimally small range around $x$.\n",
    "\n",
    "This definition satisfies the Axioms of Probability (Kolmogorov, 1933):\n",
    "1. $p(A)$ must be $\\ge 0$ for all $A$ and,\n",
    "2. The sum of probabilities of all events (or the integral of the probability density function) must be unity.\n",
    "3. If $A$ and $B$ are disjoint (independent) events, the probability of _either_ occuring is $p(A \\,{\\rm or}\\, B) = P(A) + P(B)$\n",
    "\n",
    "Note: any function satisfying Kolmogorov's axioms can be considered to describe probability, irrespective of our interpretation. That will become important later when we re-interpret probability as the degree of belief (knowledge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are \"Events\"?\n",
    "\n",
    "Events can be nearly anything:\n",
    "\n",
    "* The properties of numbers after a roll of a dice\n",
    "* The numbers on consecutive rolls of a dice\n",
    "* Measuring a particular value of position in the x direction\n",
    "* Measuring a value of position in the x-y plane (counts as two \"events\")!\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilities of Multiple Events\n",
    "\n",
    "Nearly all of our work will be about probabilities of multiple events and their relationships.\n",
    "\n",
    "Events and their relationships can be graphically visualized with Venn diagrams. If we have two events, $A$ and $B$, the possible combinations are illustrated by the following figure:\n",
    "![Figure 3.1](http://www.astroml.org/_images/fig_prob_sum_1.png)\n",
    "\n",
    "$A \\cup B$ is the *union* of sets $A$ and $B$. $A \\cap B$ is the *intersection* of sets $A$ and $B$. Nearly all of our work will be about probabilities of multiple events and their relationships.\n",
    "\n",
    "\n",
    "The probability that *either* $A$ or $B$ will happen (which could include both) is the *union*, given by\n",
    "\n",
    "$$p(A \\cup B) = p(A) + p(B) - p(A \\cap B)$$\n",
    "\n",
    "While this can be formally proven from Kolmogorov's axioms, the figure above makes it clear why the last term is necessary.  Since $A$ and $B$ overlap, we are double-counting the outcomes where *both* $A$ and $B$ happen, so we have to subtract this out.\n",
    "\n",
    "The last term $p(A \\cap B)$ is known as the **joint probability** of A and B: the probability A and B will both occur simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example:\n",
    "\n",
    "    A = rolled > 3, B = the number rolled was an even number\n",
    "    \n",
    "    P(greater than 3 or even) = P(> 3) + P(is even) - P(> 3 and even) = 3/6 + 3/6 - 2/6 = 4/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional Probability\n",
    "\n",
    "The probability that *both* $A$ and $B$ will happen, $p(A \\cap B)$, can be written as:\n",
    "\n",
    "$$p(A \\cap B) = p(A \\mid B)\\, p(B) = p(B \\mid A)\\, p(A)$$\n",
    "\n",
    "where $p(A \\mid B)$ is the the **conditional probability** of $A$ occuring *given that* $B$ has occured. This is the **definition** of conditional probability.\n",
    "\n",
    "In words: \"*The probability that both A and B have occured is equal to the probability B has occured times the probability that A will occur if B occured*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example:\n",
    "\n",
    "    A = rolled > 3, B = the number rolled was an even number\n",
    "    \n",
    "    P(greater than 3 and even) = P(> 3|if even)    * P(is even)\n",
    "\n",
    "Working it out:\n",
    "\n",
    "    - All outcomes: 1 2 3 4 5 6\n",
    "    - rolled an even number: 2 4 6\n",
    "        P(is even) = count(2 4 6) / count (1 2 3 4 5 6) = 3/6\n",
    "    - number is > 3, if even: 4 6\n",
    "        P(> 3 | if even)         = count(4 6)   / count (2 4 6)       = 2/3\n",
    "    \n",
    "    P(greater than 3 and even) = 2/3 * 3/6 = 1/3\n",
    "    \n",
    "Directly:\n",
    "\n",
    "    count(4 6) / count(1 2 3 4 5 6) = 2/6 = 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notation\n",
    "\n",
    "Different textbooks use different notation for joint probability. The following all mean the same thing:\n",
    "\n",
    "$$p(A \\cap B) = p(A,B) = p(AB) = p(A \\,{\\rm and}\\, B)$$\n",
    "\n",
    "From now on, **we will use the comma notation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probability of independent events\n",
    "\n",
    "The following is *always* true (it's the definition of conditional probability):\n",
    "\n",
    "$$p(A,B) = p(A \\mid B) \\, p(B) = p(B \\mid A) \\, p(A)$$\n",
    "\n",
    "However, if $A$ and $B$ are ***independent*** then the above simplifies to:\n",
    "\n",
    "$$p(A,B) = p(A)\\, p(B)$$\n",
    "\n",
    "Events $A$ and $B$ are independent if the outcome of $B$ does not change the possible outcomes (or their probabilities) for $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example:\n",
    "\n",
    "Let's roll the dice twice. What is the probability of obtaining 4 in the second roll (event \"A\"), if we obtained 5 in the first roll (event \"B\")?\n",
    "\n",
    "    p(second is 4, first is 5) = p(second is 4 | first is 5) * p(first is 5)\n",
    "                               = p(second is 4) * p(first is 5)\n",
    "                               = 1/6 * 1/6 = 1/36\n",
    "\n",
    "In other words, ***knowing A happened tells us nothing about whether B happened (or will happen), and vice versa***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Law of Total Probability (Theorem)\n",
    "\n",
    "![total probability](figures/total-probability-2.png)\n",
    "\n",
    "If events $B_i$ are independent and their union is the set of all possible outcomes, then the **law of total probability** says that:\n",
    "\n",
    "$$p(A) = \\sum_ip(A, B_i) = \\sum_ip(A \\mid B_i)\\, p(B_i)$$\n",
    "\n",
    "What is this? This is **the probability A will occur irrespective of which the set of events B occurs**. This is an *extremely important* concept (known as \"marginal probability\" in the continuous case), as it allows us to ignore uninteresting dependencies in the data, or apply it to cases when they're unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous case\n",
    "\n",
    "All of this generalizes to continuous processes (i.e, where events are not discrete -- say, a set of outcomes of a roll of a dice -- but continouos -- say, the result of measurement of the position of a star). The generalization is usually as simple as $\\sum_i \\rightarrow \\int dx$.\n",
    "\n",
    "### Probability density\n",
    "In the continous case, we speak of the _probability density_, $p(x)$ of the outcome being $x$. The _probability_ makes sense only when integrated over an interval, i.e.:\n",
    "\n",
    "$$ p(x_0 < x < x_1) = \\int_{x_0}^{x_1} p(x) dx$$\n",
    "\n",
    "is the probability of the value of $x$ being in some interval $(x_0, x_1)$.\n",
    "\n",
    "Example: the probability a star's brightness is $x$.\n",
    "\n",
    "### Joint probability\n",
    "\n",
    "Similarly, the joint probability of two outcomes being $x$ and $y$ is given by:\n",
    "\n",
    "$$ p(x_0 < x < x_1, y_0 < y < y_1) = \\int_{x_0}^{x_1} \\int_{y_0}^{y_1} p(x, y)\\, dx dy$$\n",
    "\n",
    "Example: the probability a star is at position $(ra, dec)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Starting with conditional probability:\n",
    "\n",
    "$$p(x, y) = p(x \\mid y)\\,p(y) = p(y \\mid x)\\,p(x)$$\n",
    "\n",
    "we can write:\n",
    "\n",
    "$$p(y \\mid x) = \\frac{p(x \\mid y)\\,p(y)}{p(x)} = \\frac{p(x \\mid y)\\,p(y)}{\\int p(x \\mid y)\\,p(y)\\,dy}$$\n",
    "\n",
    "which in words says that\n",
    "\n",
    "> the (conditional) probability of $y$ given $x$ is just the (conditional) probability of $x$ given $y$ times the (marginal) probability of $y$ divided by the (marginal) probability of $x$, where the latter is just the integral of the numerator.\n",
    "\n",
    "This is the **Bayes' rule** (a.k.a, Bayes' Theorem). It tells us **how to express $p(y|x)$ (something we'd like to know) in terms of $p(x \\mid y)$ (something we can compute).**\n",
    "\n",
    "Bayes' rule itself isn't at all controversial, though its application can be as we'll discuss later in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: LEGOs \n",
    "\n",
    "An example with LEGOs (it's awesome!):\n",
    "[https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego](https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Contingency Table\n",
    "\n",
    "Let's say that we have a test for a disease.\n",
    "\n",
    "The test can be positive ($T=1$) or negative ($T=0$). And one can either have the disease ($D=1$) or not ($D=0$).\n",
    "\n",
    "So, there are 4 possibilities:\n",
    "\n",
    "$$T=0; D=0 \\;\\;\\;  {\\rm true \\; negative}$$\n",
    "$$T=0; D=1 \\;\\;\\; {\\rm false \\; negative}$$\n",
    "$$T=1; D=0 \\;\\;\\; {\\rm false \\; positive}$$\n",
    "$$T=1; D=1 \\;\\;\\; {\\rm true \\; positive}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we \"tested\" by flipping a coin, you'd have a 50% chance of being misdiagnosed.  Not good!  But the probability of disease and the accuracy of the test presumably are not random.\n",
    "\n",
    "If the rates of false positive and false negative are:\n",
    "\n",
    "$$p(T=1|D=0) = \\epsilon_{\\rm FP}$$\n",
    "$$p(T=0|D=1) = \\epsilon_{\\rm FN}$$\n",
    "\n",
    "then the true positive and true negative rates are just:\n",
    "\n",
    "$$p(T=0| D=0) = 1-\\epsilon_{\\rm FP}$$\n",
    "$$p(T=1| D=1) = 1-\\epsilon_{\\rm FN}$$\n",
    "\n",
    "Let's assume that $\\epsilon_{\\rm FP}=0.02$ and $\\epsilon_{\\rm FN}=0.001$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In graphical form this 2x2 p(T=0 or 1|D=0 or 1) matrix is:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"http://www.astroml.org/_images/fig_contingency_table_1.png\" width=500 align>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we have **prior knowledge** regarding how prevalent is the disease (i.e., how likely is it that a random person in the population has i):\n",
    "\n",
    "$$p(D=1)=\\epsilon_D$$\n",
    "\n",
    "we should take this into account (note that then $p(D=0)=1-\\epsilon_D$).\n",
    "\n",
    "Say, $\\epsilon_D = 0.01$ (i.e., at any given time 1% of the population has the disease).\n",
    "\n",
    "Now assume that a person tested positive. What is the probability that **this specific person** has the disease?\n",
    "\n",
    "Is it 98% \n",
    "because $\\epsilon_{\\rm FP}=0.02$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "No! We can't just read $p(D=1|T=1)$ off the table because the table entry is the conditional probability of the *test* given the *data*, $p(T=1|D=1)$. What we want is the conditional probability of the *data* given the *test*, that is, $p(D=1|T=1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayes' rule then can be used to help us determine how likely it is that you have the disease if you tested positive:\n",
    "\n",
    "$$p(D=1|T=1) = \\frac{p(T=1|D=1)p(D=1)}{p(T=1)},$$\n",
    "\n",
    "where $$p(T=1) = p(T=1|D=0)p(D=0) + p(T=1|D=1)p(D=1).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore:\n",
    "\n",
    "$$p(D=1|T=1) = \\frac{(1 - \\epsilon_{FN})\\epsilon_D}{\\epsilon_{FP}(1-\\epsilon_D) + (1-\\epsilon_{FN})\\epsilon_D} \\approx \\frac{\\epsilon_D}{\\epsilon_{FP}+\\epsilon_D}$$\n",
    "\n",
    "That means that to get a reliable diagnosis, we need $\\epsilon_{FP}$ to be quite small.  (Because you *want* the probability to be close to unity if you test positive, otherwise it is a *false* positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our example, we have a disease rate of 1% ($\\epsilon_D = 0.01$) and a false positive rate of 2% ($\\epsilon_{\\rm FP}=0.02$).  \n",
    "\n",
    "So we have:\n",
    "$$p(D=1|T=1) = \\frac{0.01}{0.02+0.01} = 0.333$$\n",
    "\n",
    "Then in a sample of, e.g., 1000 people, 10 people test positive and *actually* have the disease $(1000*0.01)$, but another 20 $(1000*0.02)$ will test positive while healthy! Therefore, in that sample of 30 people who tested positive, only 1/3 has the disease (not 98%!).\n",
    "\n",
    "Same math, with often surprising results, applies to DNA tests of murder suspects..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "MWGaia-DN School (Monday)",
   "language": "python",
   "name": "mw-gaia-school-2024-pymc3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
